{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile requirements.txt\n# This file lists all the Python packages and their exact versions required for this project.\n# Using specific versions ensures reproducibility across different environments.\n\n# Deep Learning Frameworks and NLP Libraries\ntensorflow==2.15.0        # Core deep learning library by Google, enabling neural network creation and training.\ntensorflow-hub==0.14.0    # For publishing, discovering, and reusing pre-trained ML modules.\ntensorflow-text==2.15.0   # Provides TensorFlow operations for text processing (crucially, version must match core TensorFlow for compatibility).\ntransformers==4.35.0      # Hugging Face library for state-of-the-art NLP models (like BERT) for model architecture and tokenization.\ntorch==2.1.0              # PyTorch deep learning framework (often a dependency for Hugging Face Transformers internal operations).\n\n# Data Manipulation and Machine Learning Utilities\npandas==2.0.3             # Fundamental library for data manipulation and analysis using DataFrames.\nnumpy==1.26.4             # Basic numerical computing library for array operations.\nscikit-learn==1.3.0       # Machine learning library for data splitting, preprocessing, and evaluation metrics.\n\n# Visualization Libraries\nmatplotlib==3.7.2         # Basic plotting library.\nseaborn==0.12.2           # High-level statistical data visualization based on Matplotlib.\nplotly==5.17.0            # Interactive graphing library for web-based, dynamic plots.\n\n# FastAPI API Development and Server Components\nfastapi==0.104.1          # Modern, high-performance web framework for building APIs.\nuvicorn==0.24.0           # ASGI server to run FastAPI applications.\npydantic==2.4.2           # Data validation and settings management (used by FastAPI for request/response models).\npython-multipart==0.0.6   # Supports handling form data in FastAPI requests.\n\n# Web and Utility Libraries\njinja2==3.1.2             # Powerful templating engine (useful for rendering HTML in web UIs, though not directly used in API core).\naiofiles==23.2.0          # Enables asynchronous file I/O operations (for non-blocking file access in async apps).\npython-dotenv==1.0.0      # Loads environment variables from .env files (for secure management of sensitive data like API keys).\n\n# Logging and Rich Terminal Output\nloguru==0.7.2             # Simplified and powerful logging library.\nrich==13.6.0              # Library for rich text and beautiful formatting in the terminal.\n\n# Streamlit (Optional: Included for convenience as a potential future UI framework)\nstreamlit==1.28.0         # Open-source framework for building interactive web applications for ML/Data Science.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile config.py\nimport os # Module for interacting with the operating system (e.g., path operations).\nfrom pathlib import Path # Object-oriented filesystem paths, preferred for cross-platform path handling.\nfrom dataclasses import dataclass # Decorator to easily create classes that store data (config settings).\nfrom typing import Optional # For type hinting, indicating a value can be None or a specific type.\n\n@dataclass\nclass ModelConfig:\n    \"\"\"Model configuration settings, defining BERT model's core hyperparameters and properties.\"\"\"\n    model_name: str = \"bert-base-uncased\" # Name of the pre-trained BERT model from Hugging Face Transformers.\n    max_length: int = 128               # Maximum sequence length for tokenization; sequences longer are truncated, shorter are padded.\n    num_classes: int = 2                # Number of output classes for sentiment (e.g., 2 for positive/negative).\n    dropout_rate: float = 0.1           # Dropout probability applied in the classification head for regularization.\n    learning_rate: float = 2e-5         # Initial learning rate for the Adam optimizer during training.\n    batch_size: int = 16                # Number of samples processed in one forward/backward pass during training.\n    epochs: int = 3                     # Number of full passes through the entire training dataset.\n    validation_split: float = 0.2       # Proportion of the training data to be reserved for validation.\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Training configuration settings, controlling the training process behavior and callback strategies.\"\"\"\n    save_strategy: str = \"epoch\"        # Defines when to save model checkpoints (e.g., 'epoch' to save after each epoch).\n    evaluation_strategy: str = \"epoch\"  # Defines when to evaluate the model on the validation set (e.g., 'epoch').\n    logging_steps: int = 100            # How many steps between logging training progress updates.\n    save_total_limit: int = 3           # Maximum number of model checkpoints to keep. Older ones are deleted.\n    load_best_model_at_end: bool = True # If True, the model with the best validation metric is loaded at the end of training.\n    metric_for_best_model: str = \"eval_accuracy\" # The metric to monitor to determine the \"best\" model checkpoint.\n    greater_is_better: bool = True      # If True, a higher value for `metric_for_best_model` indicates a better model.\n\n@dataclass\nclass ProjectConfig:\n    \"\"\"Project paths and settings, defining the directory structure for inputs and outputs.\"\"\"\n    # Project root directory. Critically set to '/kaggle/working/' for execution within Kaggle Notebooks.\n    project_root: Path = Path(\"/kaggle/working\").absolute() \n    data_dir: Path = project_root / \"data\"     # Directory for raw and processed data files.\n    models_dir: Path = project_root / \"models\" # Directory to save trained model checkpoints (e.g., best_model.h5).\n    logs_dir: Path = project_root / \"logs\"     # Directory for application logs (e.g., app.log).\n    output_dir: Path = project_root / \"outputs\" # Directory for evaluation results, reports, and plots.\n    temp_dir: Path = project_root / \"temp\"     # Directory for temporary files created during execution.\n\n    def __post_init__(self):\n        \"\"\"\n        Special method that runs automatically after an instance of ProjectConfig is created.\n        It ensures all specified project directories exist, creating them if necessary.\n        \"\"\"\n        for dir_path in [self.data_dir, self.models_dir, self.logs_dir,\n                        self.output_dir, self.temp_dir]:\n            dir_path.mkdir(parents=True, exist_ok=True) # `parents=True` creates any missing parent directories.\n\n# Initialize configurations: Create single instances of each config class.\n# These instances hold the definitive settings and are imported across other modules.\nmodel_config = ModelConfig()\ntraining_config = TrainingConfig()\nproject_config = ProjectConfig()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile logger.py\nimport logging # Standard Python logging module (used by RichHandler internally).\nimport sys     # System-specific parameters and functions (for stderr).\nfrom loguru import logger # Simplified and powerful logging library.\nfrom rich.console import Console # Rich console for beautiful terminal output.\nfrom rich.logging import RichHandler # Loguru handler to integrate with Rich.\n\n# Import project_config to access log directory path defined in config.py.\nfrom config import project_config \n\ndef setup_logging(level: str = \"INFO\") -> None:\n    \"\"\"\n    Sets up a professional logging configuration using Loguru and Rich.\n    Logs messages to both console (formatted by Rich) and a file.\n    \n    Args:\n        level (str): Minimum logging level to display and save (e.g., \"INFO\", \"DEBUG\", \"ERROR\").\n                     Messages below this level will be ignored.\n    \"\"\"\n    \n    # Remove any default Loguru handlers to take full control over logging destinations.\n    logger.remove()\n    \n    # Add a RichHandler to direct log messages to the console (stderr).\n    # `Console(stderr=True)` directs output to the standard error stream, which is common in Jupyter environments.\n    # `rich_tracebacks=True` enhances traceback formatting for better debugging.\n    logger.add(\n        RichHandler(console=Console(stderr=True), rich_tracebacks=True), \n        # Define the format for log messages: timestamp, level, source (file:function:line), and message.\n        format=\"{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}\",\n        level=level # Set the minimum logging level for console output.\n    )\n    \n    # Add a file handler to persist log messages to a file.\n    logger.add(\n        project_config.logs_dir / \"app.log\", # Construct the full log file path using ProjectConfig.\n        rotation=\"10 MB\", # Configure log file rotation: a new file is created when the current one reaches 10 MB.\n        retention=\"7 days\", # Configure log file retention: log files older than 7 days are automatically deleted.\n        level=level, # Set the minimum logging level for file output.\n        # Define the format for log messages saved to the file.\n        format=\"{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}\" \n    )\n    \n    # Log a confirmation message that the logging setup has been completed successfully.\n    logger.info(\"Logging setup completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile data_loader.py\nimport pandas as pd # Library for data manipulation (e.g., concatenating arrays, not directly used for DataFrame operations here).\nimport numpy as np # Fundamental package for numerical computing, used for array operations like concatenation.\nimport tensorflow as tf # Core deep learning library, specifically used here for loading the IMDB dataset from Keras.\nfrom sklearn.model_selection import train_test_split # Utility for splitting datasets into random train and test subsets.\nfrom sklearn.utils import shuffle # Utility for randomizing the order of elements in lists.\nfrom typing import Tuple, List, Dict, Optional # Used for type hints, improving code readability and maintainability.\nimport re # Module for regular expressions, used for pattern matching and text manipulation (cleaning).\nimport html # Module for working with HTML entities, used for decoding HTML in text cleaning.\nfrom loguru import logger # Used for structured and informative logging of data processing steps.\n\nclass DataProcessor:\n    \"\"\"\n    Advanced data processing pipeline for sentiment analysis.\n    This class handles loading the raw IMDB dataset, cleaning text content,\n    and splitting the data into training, validation, and test sets.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initializes the DataProcessor with mappings for sentiment labels.\n        This provides a clear way to convert between string labels and numerical IDs.\n        \"\"\"\n        self.label_mapping = {\"negative\": 0, \"positive\": 1} # Maps string labels to numerical IDs.\n        self.reverse_label_mapping = {0: \"negative\", 1: \"positive\"} # Maps numerical IDs back to string labels.\n\n    def clean_text(self, text: str) -> str:\n        \"\"\"\n        Applies an advanced text cleaning pipeline to a single text string.\n        This function is crucial for standardizing input text before tokenization,\n        ensuring consistency with how the model was trained.\n\n        Args:\n            text (str): The input text string to be cleaned.\n\n        Returns:\n            str: The cleaned text string.\n        \"\"\"\n        # Ensure the input is a string; return empty if not to prevent errors.\n        if not isinstance(text, str):\n            return \"\"\n\n        # Decode HTML entities (e.g., convert '&' to '&').\n        # This prevents HTML encoding from interfering with sentiment analysis.\n        text = html.unescape(text)\n\n        # Remove HTML tags (e.g., '<br />' becomes '').\n        # HTML tags are noise for sentiment analysis.\n        text = re.sub(r'<[^>]+>', '', text)\n\n        # Normalize whitespace: replace multiple spaces, tabs, and newlines with a single space.\n        # This ensures consistent spacing and removes extra blank lines.\n        text = re.sub(r'\\s+', ' ', text)\n\n        # Remove any characters that are not alphanumeric, whitespace, or common punctuation.\n        # This helps in removing special symbols or emojis that might not be in BERT's vocabulary.\n        text = re.sub(r'[^\\w\\s.,!?;:-]', '', text)\n\n        # Convert the entire text to lowercase and remove any leading/trailing whitespace.\n        # Lowercasing helps standardize words (e.g., \"Good\" and \"good\" are treated the same).\n        text = text.lower().strip()\n\n        return text\n\n    def load_imdb_dataset(self, num_samples: Optional[int] = None) -> Tuple[List[str], List[int]]:\n        \"\"\"\n        Loads the IMDB movie review dataset directly from TensorFlow Keras datasets.\n        It then decodes the numerical review sequences back into human-readable text,\n        applies the defined cleaning process, and optionally limits the number of samples.\n\n        Args:\n            num_samples (Optional[int]): If provided, only this many samples will be loaded\n                                         from the full dataset. Useful for faster debugging\n                                         and initial development.\n\n        Returns:\n            Tuple[List[str], List[int]]: A tuple containing:\n                - List of cleaned text reviews.\n                - List of corresponding numerical sentiment labels (0 for negative, 1 for positive).\n        \"\"\"\n        logger.info(\"Loading IMDB dataset...\")\n\n        try:\n            # Load the IMDB dataset. This includes numerical sequences of reviews and their labels.\n            # The data is downloaded the first time this function is called.\n            (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()\n\n            # Retrieve the word index mapping from the IMDB dataset.\n            # This dictionary maps words to integer IDs.\n            word_index = tf.keras.datasets.imdb.get_word_index()\n            # Create a reverse mapping from integer IDs back to words.\n            # The '-3' accounts for special tokens (padding, start-of-sequence, unknown).\n            reverse_word_index = {value: key for key, value in word_index.items()}\n\n            # Nested helper function to decode a single numerical review sequence back to text.\n            def decode_review(encoded_review):\n                return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n\n            # Convert all numerical training and testing reviews into text strings.\n            x_train_text = [decode_review(review) for review in x_train]\n            x_test_text = [decode_review(review) for review in x_test]\n\n            # Combine all texts and labels into single lists for unified processing.\n            all_texts = x_train_text + x_test_text\n            all_labels = np.concatenate([y_train, y_test])\n\n            # Apply the text cleaning function to all combined texts.\n            logger.info(\"Cleaning text data...\")\n            cleaned_texts = [self.clean_text(text) for text in all_texts]\n\n            # Filter out reviews that became too short or empty after cleaning.\n            # This prevents very short, uninformative texts from being processed.\n            valid_indices = [i for i, text in enumerate(cleaned_texts) if len(text.strip()) > 10]\n            cleaned_texts = [cleaned_texts[i] for i in valid_indices]\n            all_labels = [all_labels[i] for i in valid_indices]\n\n            # Optionally limit the number of samples for faster experimentation.\n            if num_samples and num_samples < len(cleaned_texts):\n                # Shuffle the data before slicing to ensure randomness in selected samples.\n                cleaned_texts, all_labels = shuffle(cleaned_texts, all_labels, random_state=42)\n                cleaned_texts = cleaned_texts[:num_samples] # Select first `num_samples`.\n                all_labels = all_labels[:num_samples]\n\n            logger.info(f\"Loaded {len(cleaned_texts)} samples\")\n            return cleaned_texts, all_labels\n\n        except Exception as e:\n            # Log any errors that occur during dataset loading and re-raise them.\n            logger.error(f\"Error loading IMDB dataset: {e}\")\n            raise\n\n    def create_data_splits(self, texts: List[str], labels: List[int],\n                          test_size: float = 0.2, val_size: float = 0.1) -> Dict:\n        \"\"\"\n        Splits the provided texts and labels into distinct training, validation, and test datasets.\n        It uses a stratified splitting approach to ensure that the class distribution (positive/negative)\n        is preserved in each subset.\n\n        Args:\n            texts (List[str]): A list of cleaned text samples.\n            labels (List[int]): A list of corresponding numerical sentiment labels.\n            test_size (float): The proportion of the dataset to allocate to the final test set (e.g., 0.2 for 20%).\n            val_size (float): The proportion of the remaining data (after test split) to allocate to the validation set.\n\n        Returns:\n            Dict: A dictionary containing three keys ('train', 'validation', 'test'),\n                  each mapping to another dictionary with 'texts' and 'labels' for that split.\n        \"\"\"\n        logger.info(\"Creating data splits...\")\n\n        # First split: Separate out the final test set.\n        # `stratify=labels` ensures that the proportion of classes (0s and 1s) is the same in both X_temp and X_test.\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            texts, labels, test_size=test_size, random_state=42, stratify=labels\n        )\n\n        # Second split: Divide the remaining data (X_temp, y_temp) into training and validation sets.\n        # The validation size is adjusted because it's a split of the *remaining* data, not the original full dataset.\n        val_size_adjusted = val_size / (1 - test_size)\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n        )\n\n        # Organize the split datasets into a dictionary for easy access.\n        data_splits = {\n            'train': {'texts': X_train, 'labels': y_train},\n            'validation': {'texts': X_val, 'labels': y_val},\n            'test': {'texts': X_test, 'labels': y_test}\n        }\n\n        logger.info(f\"Data splits - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n        return data_splits\n\n    def create_tf_dataset(self, texts: List[str], labels: List[int],\n                         batch_size: int, shuffle: bool = True) -> tf.data.Dataset:\n        \"\"\"\n        Converts Python lists of text samples and their corresponding labels into a\n        highly performant TensorFlow `tf.data.Dataset` object. This format is\n        optimized for feeding data to TensorFlow models during training.\n\n        Args:\n            texts (List[str]): A list of text samples.\n            labels (List[int]): A list of corresponding numerical labels.\n            batch_size (int): The number of elements (samples) to include in each batch\n                              of the dataset.\n            shuffle (bool): If True, the dataset will be shuffled. Recommended for training data.\n\n        Returns:\n            tf.data.Dataset: A TensorFlow dataset, which is batched and prefetched\n                             for efficient data loading and processing during model training.\n        \"\"\"\n        # Create a tf.data.Dataset from slices of the input texts and labels.\n        dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n\n        # Shuffle the dataset if specified.\n        # `buffer_size` is used for shuffling, ensuring efficient shuffling.\n        if shuffle:\n            dataset = dataset.shuffle(buffer_size=1000, seed=42)\n\n        # Batch the dataset into smaller chunks for efficient processing by the model.\n        dataset = dataset.batch(batch_size)\n        # Use prefetching to allow the data pipeline to prepare batches in the background\n        # while the model is processing the current batch, improving performance.\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n        return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile model.py\nimport tensorflow as tf # Core deep learning library for building and training models.\nimport tensorflow_hub as hub # For reusable machine learning modules.\nfrom transformers import TFBertModel, BertTokenizer # Hugging Face BERT model and tokenizer for TensorFlow.\nfrom typing import Dict, Any, Optional # For type hinting.\nfrom loguru import logger # For logging messages.\n\n# Import model_config to access model-specific hyperparameters from config.py.\nfrom config import ModelConfig \n\nclass BERTSentimentClassifier(tf.keras.Model):\n    \"\"\"\n    Professional BERT-based sentiment classifier model.\n    This class inherits from tf.keras.Model, allowing for a custom, trainable Keras model.\n    It encapsulates the BERT base and a custom classification head.\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"bert-base-uncased\", # Name of the pre-trained BERT model to load (e.g., 'bert-base-uncased').\n                 num_classes: int = 2,                 # Number of output sentiment classes (e.g., 2 for positive/negative).\n                 dropout_rate: float = 0.1,             # Dropout rate for regularization in the classification layers.\n                 max_length: int = 128,                 # Maximum sequence length for input tokens.\n                 **kwargs):                             # Allows passing additional keyword arguments to the base class.\n        super().__init__(**kwargs) # Initialize the base Keras Model class with any passed kwargs.\n        \n        self.model_name = model_name\n        self.num_classes = num_classes\n        self.dropout_rate = dropout_rate\n        self.max_length = max_length\n        \n        # Initialize the tokenizer corresponding to the pre-trained BERT model.\n        # This tokenizer converts text into numerical input IDs and attention masks.\n        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n        \n        # Load the pre-trained TFBertModel (TensorFlow version of BERT).\n        # This forms the powerful backbone of our sentiment classifier.\n        self.bert = TFBertModel.from_pretrained(model_name)\n        # Define a Dropout layer to prevent overfitting during training.\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        # Define the final classification layer.\n        # `softmax` activation outputs probabilities for each class, summing to 1.\n        self.classifier = tf.keras.layers.Dense(\n            num_classes, \n            activation='softmax', \n            name='classifier'     # Assign a name to the layer for better visualization/debugging.\n        )\n        \n        logger.info(f\"Initialized BERT model: {model_name}\")\n    \n    def tokenize_texts(self, texts):\n        \"\"\"\n        Tokenizes input texts using the pre-trained BERT tokenizer.\n        This prepares raw text for input into the BERT model.\n        \n        Args:\n            texts: A list of text strings to be tokenized.\n            \n        Returns:\n            A dictionary containing tokenized inputs as TensorFlow tensors\n            (input_ids, attention_mask, etc.).\n        \"\"\"\n        return self.tokenizer(\n            texts,\n            padding=True,              # Pads sequences to the `max_length` or the longest sequence in the batch.\n            truncation=True,           # Truncates sequences longer than `max_length`.\n            max_length=self.max_length, # Uses the `max_length` defined in model configuration.\n            return_tensors='tf'        # Ensures the output is in TensorFlow tensor format.\n        )\n    \n    def call(self, inputs, training=False):\n        \"\"\"\n        Defines the forward pass logic of the model.\n        This method is called when the model is executed (e.g., during training or prediction).\n        \n        Args:\n            inputs: A dictionary of tokenized inputs (typically 'input_ids' and 'attention_mask').\n            training (bool): A boolean indicating whether the model is currently in training mode.\n                             This is used to control the behavior of layers like Dropout (active during training).\n        \n        Returns:\n            tf.Tensor: The output logits (raw scores) or probabilities from the classification layer.\n        \"\"\"\n        # Pass the input IDs and attention mask through the BERT model.\n        # `training` argument is passed to control BERT's internal dropout layers.\n        bert_outputs = self.bert(inputs, training=training)\n        \n        # Extract the pooled output. For classification tasks, this typically represents\n        # the aggregated information of the entire sequence, usually from the [CLS] token.\n        pooled_output = bert_outputs.pooler_output\n        \n        # Apply the dropout layer. It is only active when `training` is True.\n        pooled_output = self.dropout(pooled_output, training=training)\n        \n        # Pass the pooled output through the final classification layer.\n        logits = self.classifier(pooled_output)\n        \n        return logits\n    \n    def get_config(self):\n        \"\"\"\n        Returns the model's configuration parameters.\n        This method is required for Keras to correctly serialize and deserialize the model.\n        \"\"\"\n        return {\n            'model_name': self.model_name,\n            'num_classes': self.num_classes,\n            'dropout_rate': self.dropout_rate,\n            'max_length': self.max_length\n        }\n\nclass BERTModelBuilder:\n    \"\"\"\n    A static helper class for building BERT-based models.\n    It provides a clean interface to construct model instances using the Keras Functional API.\n    \"\"\"\n    \n    @staticmethod\n    def build_functional_model(model_config: ModelConfig) -> tf.keras.Model:\n        \"\"\"\n        Builds a BERT-based sentiment classification model using the Keras Functional API.\n        The Functional API is preferred for its flexibility in defining complex architectures.\n        \n        Args:\n            model_config (ModelConfig): A configuration object containing hyperparameters\n                                        like model name, max length, number of classes, and dropout rate.\n                                        \n        Returns:\n            tf.keras.Model: A compiled Keras model ready for training or prediction.\n        \"\"\"\n        \n        # Define the input layers for the BERT model: input_ids and attention_mask.\n        # `input_ids` are the numerical representations of tokens.\n        input_ids = tf.keras.layers.Input(\n            shape=(model_config.max_length,), # Input shape is (sequence_length,), batch size is implicit (None).\n            dtype=tf.int32,                   # Data type for token IDs is integer.\n            name='input_ids'                  # A name for the input layer, useful for model summaries and debugging.\n        )\n        # `attention_mask` indicates which tokens are real and which are padding, crucial for BERT.\n        attention_mask = tf.keras.layers.Input(\n            shape=(model_config.max_length,), \n            dtype=tf.int32, \n            name='attention_mask'\n        )\n        \n        # Load the pre-trained TensorFlow BERT model from Hugging Face.\n        # This is the backbone of our classification model.\n        bert = TFBertModel.from_pretrained(model_config.model_name)\n        # Pass the input layers through the BERT model.\n        bert_outputs = bert(input_ids, attention_mask=attention_mask)\n        \n        # Extract the pooled output from BERT. This is typically the representation of the [CLS] token,\n        # which is used as the aggregate representation of the entire input sequence for classification.\n        pooled_output = bert_outputs.pooler_output\n        \n        # Add custom classification layers on top of BERT's output.\n        # Dropout layer for regularization to prevent overfitting.\n        x = tf.keras.layers.Dropout(model_config.dropout_rate)(pooled_output) \n        # An additional Dense layer with ReLU activation for non-linear transformation.\n        x = tf.keras.layers.Dense(128, activation='relu')(x) \n        # Another Dropout layer.\n        x = tf.keras.layers.Dropout(model_config.dropout_rate)(x)\n        # Final output Dense layer with softmax activation for multi-class probability distribution.\n        outputs = tf.keras.layers.Dense(\n            model_config.num_classes, \n            activation='softmax' \n        )(x)\n        \n        # Construct the full Keras Model by specifying its inputs and outputs.\n        model = tf.keras.Model(\n            inputs=[input_ids, attention_mask], # List of input layers.\n            outputs=outputs,                   # Output tensor from the final layer.\n            name='bert_sentiment_classifier'   # A descriptive name for the entire model.\n        )\n        \n        return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile trainer.py\nimport tensorflow as tf # Core deep learning library for model training.\nfrom tensorflow.keras.optimizers import Adam # Optimizer for updating model weights.\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy # Loss function for integer labels.\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy # Metric to track accuracy for integer labels.\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint,       # Callback to save the best model during training.\n    EarlyStopping,         # Callback to stop training early if validation metric plateaus.\n    ReduceLROnPlateau,     # Callback to reduce learning rate when a metric stops improving.\n    TensorBoard,           # Callback for visualizing training progress.\n    CSVLogger              # Callback to save training history to a CSV file.\n)\nfrom sklearn.metrics import classification_report, confusion_matrix # For detailed evaluation metrics.\nimport numpy as np # For numerical operations, especially converting labels to NumPy arrays.\nimport matplotlib.pyplot as plt # For plotting (though mostly handled by Plotly for interactive plots).\nimport seaborn as sns # For enhanced statistical data visualization (often used with Matplotlib).\nfrom typing import Dict, List, Tuple, Optional # For type hinting.\nfrom loguru import logger # For structured logging.\nimport json # For saving training history and configurations to JSON files.\nimport time # For measuring training time.\n\n# Import configuration objects from config.py.\nfrom config import ModelConfig, TrainingConfig, project_config\n\nclass BERTTrainer:\n    \"\"\"\n    Professional BERT training pipeline.\n    Manages model compilation, callbacks setup, and the training loop.\n    \"\"\"\n    \n    def __init__(self, \n                 model: tf.keras.Model,          # The Keras model to be trained.\n                 model_config: ModelConfig,       # Model configuration object.\n                 training_config: TrainingConfig): # Training configuration object.\n        # Call the constructor of the parent class (no explicit parent in this class definition,\n        # but it's good practice or might be inherited implicitly from object).\n        # This line was previously `super().__init__(**kwargs)` which was incorrect without a proper parent setup.\n        super().__init__() \n        self.model = model\n        self.model_config = model_config\n        self.training_config = training_config\n        self.history = None # To store training history (loss, accuracy per epoch).\n        self.tokenizer = None # To store the BERT tokenizer.\n\n        # Initialize tokenizer from Hugging Face Transformers.\n        # Imported here to avoid circular dependencies if model.py also imports trainer,\n        # ensuring the tokenizer is available when the trainer is instantiated.\n        from transformers import BertTokenizer \n        self.tokenizer = BertTokenizer.from_pretrained(model_config.model_name)\n\n        logger.info(\"BERT Trainer initialized\")\n    \n    def prepare_dataset(self, texts: List[str], labels: List[int]) -> tf.data.Dataset:\n        \"\"\"\n        Prepares a TensorFlow dataset from texts and labels, including tokenization.\n        Note: This method is currently NOT directly used in the `train` method's workflow\n        (tokenization is handled directly within `train`). It might be a remnant\n        from an alternative data pipeline design.\n        \"\"\"\n        def tokenize_function(texts, labels):\n            # Tokenize texts, converting them to numerical IDs and attention masks.\n            tokenized = self.tokenizer(\n                texts.numpy().tolist(), # Convert TensorFlow tensors back to Python list for tokenizer processing.\n                padding=True,           # Pad sequences to a uniform length.\n                truncation=True,        # Truncate sequences that are too long.\n                max_length=self.model_config.max_length, # Use max_length from model config.\n                return_tensors='tf'     # Ensure output is in TensorFlow tensor format.\n            )\n            return {\n                'input_ids': tokenized['input_ids'],\n                'attention_mask': tokenized['attention_mask']\n            }, labels\n        \n        # Create a TensorFlow dataset from input texts and labels.\n        dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n        \n        # The following lines (`if shuffle:` and `dataset = dataset.batch(...)`)\n        # contain variables (`shuffle`, `batch_size`) that are not defined in this method's scope.\n        # If this method were to be used, these would need to be passed as arguments or accessed from config.\n        if shuffle: # This `shuffle` variable is not defined in this method's scope.\n            dataset = dataset.shuffle(buffer_size=1000, seed=42)\n        dataset = dataset.batch(batch_size) # This `batch_size` variable is not defined in this method's scope.\n        dataset = dataset.prefetch(tf.data.AUTOTUNE) # Optimize data loading by prefetching batches.\n        return dataset\n\n    def compile_model(self):\n        \"\"\"\n        Compiles the Keras model with a specified optimizer, loss function, and metrics.\n        This prepares the model for the training process before fitting the data.\n        \"\"\"\n        optimizer = Adam(learning_rate=self.model_config.learning_rate) # Use the Adam optimizer with the configured learning rate.\n        # Define the loss function for multi-class classification with integer labels.\n        # `from_logits=False` because our model's final layer uses `softmax` activation (outputs probabilities).\n        loss = SparseCategoricalCrossentropy(from_logits=False) \n        metrics = [SparseCategoricalAccuracy(name='accuracy')] # Track sparse categorical accuracy during training.\n        \n        # Compile the model with the defined optimizer, loss, and metrics.\n        self.model.compile(\n            optimizer=optimizer,\n            loss=loss,\n            metrics=metrics\n        )\n        logger.info(\"Model compiled successfully\")\n    \n    def setup_callbacks(self) -> List[tf.keras.callbacks.Callback]: # Added `self` as the first argument.\n        \"\"\"\n        Sets up a list of Keras Callbacks to enhance, monitor, and control the training process.\n        This includes saving the best model, early stopping, learning rate reduction, and logging.\n        \n        Returns:\n            List[tf.keras.callbacks.Callback]: A list of configured Keras callback instances.\n        \"\"\"\n        callbacks = []\n        \n        # ModelCheckpoint: Saves the model's weights or entire model at specific points.\n        checkpoint_path = project_config.models_dir / \"best_model.h5\" # Define the path where the best model will be saved.\n        checkpoint = ModelCheckpoint(\n            filepath=str(checkpoint_path), # File path for saving the model.\n            monitor='val_accuracy',        # Metric to monitor for improvement (validation accuracy).\n            save_best_only=True,           # Only save the model if the monitored metric improves.\n            save_weights_only=False,       # Save the entire model (architecture + weights).\n            mode='max',                    # 'max' indicates that higher 'val_accuracy' is better.\n            verbose=1                      # Display messages when a model is saved.\n        )\n        callbacks.append(checkpoint)\n        \n        # EarlyStopping: Stops training automatically if the monitored metric stops improving.\n        early_stopping = EarlyStopping(\n            monitor='val_accuracy',        # Monitor validation accuracy.\n            patience=2,                    # Number of epochs to wait for improvement before stopping.\n            restore_best_weights=True,     # Reverts model weights to the best performing epoch.\n            mode='max',                    # 'max' indicates that higher 'val_accuracy' is better.\n            verbose=1                      # Display messages when early stopping is triggered.\n        )\n        callbacks.append(early_stopping)\n        \n        # ReduceLROnPlateau: Reduces the learning rate when the monitored metric plateaus.\n        reduce_lr = ReduceLROnPlateau(\n            monitor='val_loss', # Monitor validation loss.\n            factor=0.5,         # Factor by which the learning rate will be reduced (new_lr = old_lr * factor).\n            patience=1,         # Number of epochs with no improvement after which learning rate will be reduced.\n            min_lr=1e-7,        # Lower bound on the learning rate.\n            mode='min',         # 'min' indicates that lower 'val_loss' is better.\n            verbose=1           # Display messages when learning rate is reduced.\n        )\n        callbacks.append(reduce_lr)\n        \n        # TensorBoard: A visualization toolkit for TensorFlow to inspect training runs.\n        tensorboard = TensorBoard(\n            log_dir=str(project_config.logs_dir / \"tensorboard\"), # Directory for TensorBoard logs.\n            histogram_freq=1, # Compute histograms for weights, biases, and activations every epoch.\n            write_graph=True, # Visualize the model's computation graph.\n            update_freq='epoch' # How often TensorBoard logs are updated.\n        )\n        callbacks.append(tensorboard)\n        \n        # CSVLogger: Streams epoch results to a CSV file.\n        csv_logger = CSVLogger(\n            str(project_config.logs_dir / \"training_log.csv\"), # Path to the CSV log file.\n            append=True # If True, append results to the file if it already exists.\n        )\n        callbacks.append(csv_logger)\n        \n        return callbacks\n    \n    def train(self, \n              train_data: Dict,     # Dictionary containing training texts and their corresponding labels.\n              val_data: Dict) -> tf.keras.callbacks.History: # Dictionary containing validation texts and their labels.\n        \"\"\"\n        Executes the training loop for the BERT model using the prepared data.\n        \n        Args:\n            train_data (Dict): A dictionary with 'texts' (list of strings) and 'labels' (list of integers)\n                                for the training set.\n            val_data (Dict): A dictionary with 'texts' (list of strings) and 'labels' (list of integers)\n                             for the validation set.\n            \n        Returns:\n            tf.keras.callbacks.History: A Keras History object, containing records of training loss values\n                                        and metrics values at successive epochs.\n        \"\"\"\n        logger.info(\"Starting model training...\")\n        start_time = time.time() # Record the start time to calculate total training duration.\n        \n        # Extract text lists and label lists from the input data dictionaries.\n        train_texts = train_data['texts']\n        train_labels = train_data['labels']\n        val_texts = val_data['texts']\n        val_labels = val_data['labels']\n        \n        # Tokenize training data using the BERT tokenizer.\n        logger.info(\"Tokenizing training data...\")\n        train_encodings = self.tokenizer(\n            train_texts,\n            padding=True,                  # Pads sequences to `max_length` or the longest in the batch.\n            truncation=True,               # Truncates sequences longer than `max_length`.\n            max_length=self.model_config.max_length, # Uses the `max_length` specified in ModelConfig.\n            return_tensors='tf'            # Returns TensorFlow tensors for input_ids and attention_mask.\n        )\n        \n        # Tokenize validation data using the BERT tokenizer.\n        logger.info(\"Tokenizing validation data...\")\n        val_encodings = self.tokenizer(\n            val_texts,\n            padding=True,\n            truncation=True,\n            max_length=self.model_config.max_length,\n            return_tensors='tf'\n        )\n        \n        # Compile the model (if it hasn't been compiled yet) using defined optimizer, loss, and metrics.\n        self.compile_model()\n        \n        # Setup and retrieve the list of Keras callbacks for this training run.\n        callbacks = self.setup_callbacks()\n        \n        # Start the actual model training process.\n        self.history = self.model.fit(\n            x=[train_encodings['input_ids'], train_encodings['attention_mask']], # BERT requires input_ids and attention_mask.\n            y=np.array(train_labels), # Convert labels to a NumPy array.\n            validation_data=( # Provide validation data for monitoring training progress.\n                [val_encodings['input_ids'], val_encodings['attention_mask']], \n                np.array(val_labels)\n            ), \n            epochs=self.model_config.epochs, # Number of epochs from ModelConfig.\n            batch_size=self.model_config.batch_size, # Batch size from ModelConfig.\n            callbacks=callbacks, # Apply the configured callbacks during training.\n            verbose=1 # Display training progress in detail for each epoch.\n        )\n        \n        training_time = time.time() - start_time # Calculate the total time taken for training.\n        logger.info(f\"Training completed in {training_time:.2f} seconds\")\n        \n        return self.history # Return the training history object.\n    \n    def save_training_artifacts(self):\n        \"\"\"\n        Saves important artifacts from the training process:\n        - The training history (loss and metrics per epoch) as a JSON file.\n        - The model configuration (ModelConfig and TrainingConfig) as a JSON file.\n        This is crucial for reproducibility and for analyzing past training runs.\n        \"\"\"\n        \n        # Save training history if the `history` object is available (meaning training occurred).\n        if self.history:\n            history_path = project_config.output_dir / \"training_history.json\"\n            \n            # Prepare the training history for JSON serialization.\n            # This step converts NumPy/TensorFlow float32 values (which are not directly JSON serializable)\n            # into standard Python floats. This resolved the `TypeError: Object of type float32 is not JSON serializable`\n            # error that occurred when attempting to save the history dictionary.\n            serializable_history = {}\n            for key, value_list in self.history.history.items():\n                serializable_history[key] = [float(val) for val in value_list] # Convert each float32 value in lists to Python float.\n            \n            # Write the serializable history to a JSON file with pretty printing (indent=2).\n            with open(history_path, 'w') as f:\n                json.dump(serializable_history, f, indent=2) \n            logger.info(f\"Training history saved to {history_path}\")\n        \n        # Save the combined model and training configurations.\n        config_path = project_config.output_dir / \"model_config.json\"\n        config_dict = {\n            'model_config': self.model_config.__dict__, # Convert ModelConfig dataclass instance to a dictionary.\n            'training_config': self.training_config.__dict__ # Convert TrainingConfig dataclass instance to a dictionary.\n        }\n        # Write the configuration dictionary to a JSON file.\n        with open(config_path, 'w') as f:\n            json.dump(config_dict, f, indent=2)\n        logger.info(f\"Model configuration saved to {config_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile evaluator.py\nimport numpy as np # For numerical operations and array manipulation.\nimport matplotlib.pyplot as plt # Basic plotting functions (used by seaborn).\nimport seaborn as sns # High-level statistical plotting library.\nfrom sklearn.metrics import ( # Various metrics for model evaluation.\n    accuracy_score,           # Overall classification accuracy.\n    precision_score,          # Precision for positive and negative classes.\n    recall_score,             # Recall for positive and negative classes.\n    f1_score,                 # F1-score (harmonic mean of precision and recall).\n    classification_report,    # Detailed report of precision, recall, f1-score for each class.\n    confusion_matrix,         # Matrix showing correct and incorrect predictions.\n    roc_auc_score,            # Area Under the Receiver Operating Characteristic (ROC) Curve.\n    roc_curve                 # Data points for plotting the ROC curve.\n)\nimport plotly.graph_objects as go # For creating interactive graph objects (e.g., scatter plots).\nimport plotly.express as px # Simplified interface for Plotly for quick plots.\nfrom plotly.subplots import make_subplots # For creating subplots in Plotly.\nfrom typing import Dict, List, Tuple, Optional # For type hints, improving code readability.\nfrom loguru import logger # For structured and informative logging.\n\n# Crucial import: TensorFlow is needed for Keras model type hints and model operations.\nimport tensorflow as tf \n# Import project and model configurations from config.py.\nfrom config import project_config, model_config \n\nclass ModelEvaluator:\n    \"\"\"\n    Comprehensive model evaluation suite.\n    This class is responsible for making predictions, calculating various performance metrics,\n    and generating interactive visualizations of the model's performance.\n    \"\"\"\n    \n    # The __init__ method is designed to directly receive the already initialized model and tokenizer.\n    # This prevents redundant loading/initialization of the tokenizer, which resolves issues like\n    # `NameError: name 'tf' is not defined` and `HFValidationError` seen in earlier iterations.\n    def __init__(self, model: tf.keras.Model, tokenizer): # The trained Keras model, and the pre-trained tokenizer.\n        self.model = model\n        self.tokenizer = tokenizer # Stores the tokenizer provided (e.g., from BERTTrainer).\n        self.class_names = ['Negative', 'Positive'] # Defines human-readable class names for reports and plots.\n        logger.info(\"Model Evaluator initialized with provided tokenizer\") # Log confirmation of initialization.\n    \n    def predict(self, texts: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Makes sentiment predictions on a list of raw text strings using the loaded BERT model.\n        The texts are tokenized, then passed through the model.\n        \n        Args:\n            texts (List[str]): A list of text strings for which sentiment predictions are required.\n            \n        Returns:\n            Tuple[np.ndarray, np.ndarray]: A tuple containing:\n                - predictions (np.ndarray): The raw probability scores output by the model for each class.\n                - predicted_classes (np.ndarray): The numerical class label (0 or 1) with the highest probability.\n        \"\"\"\n        # Tokenize the input texts using the tokenizer.\n        # This converts text into numerical input IDs and attention masks expected by BERT.\n        encodings = self.tokenizer(\n            texts,\n            padding=True,              # Pads sequences to `max_length` or the longest in the batch.\n            truncation=True,           # Truncates sequences longer than `max_length`.\n            max_length=model_config.max_length, # Uses the `max_length` specified in ModelConfig for consistent input shape.\n            return_tensors='tf'        # Ensures the output is in TensorFlow tensor format.\n        )\n        \n        # Get predictions from the model by passing the tokenized inputs.\n        # The model expects separate tensors for input_ids and attention_mask.\n        predictions = self.model.predict([\n            encodings['input_ids'],\n            encodings['attention_mask']\n        ])\n        \n        # Determine the predicted class by finding the index of the highest probability.\n        predicted_classes = np.argmax(predictions, axis=1)\n        \n        return predictions, predicted_classes # Return raw probabilities and the predicted class labels.\n    \n    def evaluate_model(self, test_data: Dict) -> Dict:\n        \"\"\"\n        Performs a comprehensive evaluation of the model's performance on the test data.\n        It calculates various standard metrics and generates a detailed classification report\n        and confusion matrix.\n        \n        Args:\n            test_data (Dict): A dictionary containing 'texts' (list of strings) and 'labels' (list of integers)\n                              for the test dataset.\n            \n        Returns:\n            Dict: A dictionary containing:\n                - 'metrics': A dictionary of scalar performance metrics (accuracy, precision, recall, f1-score, ROC AUC).\n                - 'classification_report': A detailed report per class (precision, recall, f1-score, support).\n                - 'confusion_matrix': A NumPy array representing the confusion matrix.\n                - 'predictions': Raw probability predictions from the model.\n                - 'predicted_classes': The numerical class labels predicted by the model.\n                - 'true_labels': The actual numerical class labels from the test data.\n        \"\"\"\n        logger.info(\"Starting model evaluation...\")\n        \n        test_texts = test_data['texts'] # Extract test texts.\n        test_labels = np.array(test_data['labels']) # Extract true labels and convert to NumPy array.\n        \n        # Get predictions (probabilities and predicted class labels) for the test texts.\n        probabilities, predicted_classes = self.predict(test_texts)\n        \n        # Calculate key performance metrics using scikit-learn.\n        metrics = {\n            'accuracy': accuracy_score(test_labels, predicted_classes), # Overall accuracy.\n            'precision': precision_score(test_labels, predicted_classes, average='weighted'), # Weighted average precision.\n            'recall': recall_score(test_labels, predicted_classes, average='weighted'),     # Weighted average recall.\n            'f1_score': f1_score(test_labels, predicted_classes, average='weighted'),       # Weighted average F1-score.\n            'roc_auc': roc_auc_score(test_labels, probabilities[:, 1]) # ROC AUC score for the positive class.\n        }\n        \n        # Generate a detailed classification report, using human-readable class names.\n        class_report = classification_report(\n            test_labels, predicted_classes, \n            target_names=self.class_names, # Use 'Negative', 'Positive' for clarity.\n            output_dict=True # Return the report as a dictionary.\n        )\n        \n        # Compute the confusion matrix.\n        cm = confusion_matrix(test_labels, predicted_classes)\n        \n        # Compile all evaluation results into a single dictionary.\n        results = {\n            'metrics': metrics,\n            'classification_report': class_report,\n            'confusion_matrix': cm,\n            'predictions': probabilities,\n            'predicted_classes': predicted_classes,\n            'true_labels': test_labels\n        }\n        \n        logger.info(f\"Evaluation completed. Accuracy: {metrics['accuracy']:.4f}\") # Log final accuracy.\n        return results\n    \n    def plot_training_history(self, history: tf.keras.callbacks.History):\n        \"\"\"\n        Generates and saves an interactive Plotly graph visualizing the model's training history,\n        including training and validation accuracy and loss over epochs.\n        \n        Args:\n            history (tf.keras.callbacks.History): The history object returned by `model.fit()`,\n                                                  containing epoch-wise training metrics.\n        \"\"\"\n        \n        # Create a subplot figure with two columns for accuracy and loss.\n        fig = make_subplots(\n            rows=1, cols=2,\n            subplot_titles=['Model Accuracy', 'Model Loss'], # Titles for each subplot.\n            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]] # Standard layout.\n        )\n        \n        epochs = range(1, len(history.history['accuracy']) + 1) # Generate x-axis values for epochs.\n        \n        # Add a trace for Training Accuracy.\n        fig.add_trace(\n            go.Scatter(\n                x=list(epochs), y=history.history['accuracy'], # x-axis: epochs, y-axis: accuracy values.\n                mode='lines+markers', name='Training Accuracy', # Display lines and markers.\n                line=dict(color='#1f77b4', width=3) # Custom line color and width.\n            ),\n            row=1, col=1 # Position this trace in the first subplot.\n        )\n        \n        # Add a trace for Validation Accuracy.\n        fig.add_trace(\n            go.Scatter(\n                x=list(epochs), y=history.history['val_accuracy'],\n                mode='lines+markers', name='Validation Accuracy',\n                line=dict(color='#ff7f0e', width=3)\n            ),\n            row=1, col=1\n        )\n        \n        # Add a trace for Training Loss.\n        fig.add_trace(\n            go.Scatter(\n                x=list(epochs), y=history.history['loss'],\n                mode='lines+markers', name='Training Loss',\n                line=dict(color='#1f77b4', width=3)\n            ),\n            row=1, col=2 # Position this trace in the second subplot.\n        )\n        \n        # Add a trace for Validation Loss.\n        fig.add_trace(\n            go.Scatter(\n                x=list(epochs), y=history.history['val_loss'],\n                mode='lines+markers', name='Validation Loss',\n                line=dict(color='#ff7f0e', width=3)\n            ),\n            row=1, col=2\n        )\n        \n        # Update the overall layout of the Plotly figure.\n        fig.update_layout(\n            title=\"Training History\",\n            height=500,\n            showlegend=True,\n            template=\"plotly_white\" # Use a clean, white-themed template.\n        )\n        \n        # Update axis titles for clarity.\n        fig.update_xaxes(title_text=\"Epoch\")\n        fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n        fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n        \n        # Save the interactive plot as an HTML file in the specified output directory.\n        fig.write_html(str(project_config.output_dir / \"training_history.html\"))\n        # fig.show() # This line is commented out as `fig.show()` creates pop-up windows which are generally\n                   # not supported or desirable in non-interactive notebook environments like Kaggle.\n    \n    def plot_confusion_matrix(self, cm: np.ndarray):\n        \"\"\"\n        Generates and saves an interactive Plotly heatmap visualization of the confusion matrix.\n        This helps in understanding the types of errors the model makes (e.g., false positives, false negatives).\n        \n        Args:\n            cm (np.ndarray): The confusion matrix array, typically a 2x2 matrix for binary classification.\n        \"\"\"\n        \n        # Create a Plotly Express imshow (heatmap) figure.\n        fig = px.imshow(\n            cm,\n            labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"), # Labels for axes and color bar.\n            x=self.class_names, # Labels for the predicted classes on the x-axis.\n            y=self.class_names, # Labels for the actual classes on the y-axis.\n            text_auto=True,     # Automatically display the value of each cell on the heatmap.\n            aspect=\"auto\",      # Adjusts the aspect ratio of the heatmap automatically.\n            color_continuous_scale=\"Blues\" # Use a blue color scale for the heatmap.\n        )\n        \n        fig.update_layout(\n            title=\"Confusion Matrix\", # Set the title of the plot.\n            width=500,                # Set the width of the plot.\n            height=500                # Set the height of the plot.\n        )\n        \n        # Save the interactive confusion matrix plot as an HTML file.\n        fig.write_html(str(project_config.output_dir / \"confusion_matrix.html\"))\n        # fig.show() # Commented out for Notebook compatibility.\n    \n    def plot_roc_curve(self, y_true: np.ndarray, y_proba: np.ndarray):\n        \"\"\"\n        Generates and saves an interactive Plotly plot of the Receiver Operating Characteristic (ROC) curve.\n        The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier\n        as its discrimination threshold is varied. AUC (Area Under the Curve) provides a single metric\n        to summarize the overall performance.\n        \n        Args:\n            y_true (np.ndarray): True binary labels (e.g., 0s and 1s).\n            y_proba (np.ndarray): Predicted probabilities of the positive class (e.g., probabilities for class 1).\n        \"\"\"\n        \n        # Calculate the False Positive Rate (FPR), True Positive Rate (TPR), and thresholds for the ROC curve.\n        fpr, tpr, _ = roc_curve(y_true, y_proba[:, 1]) # Compute ROC for the positive class (column 1 of probabilities).\n        # Calculate the Area Under the ROC Curve (AUC score).\n        auc_score = roc_auc_score(y_true, y_proba[:, 1])\n        \n        fig = go.Figure() # Create a new Plotly Figure object.\n        \n        # Add the main ROC Curve trace to the figure.\n        fig.add_trace(go.Scatter(\n            x=fpr, y=tpr,                                 # FPR on x-axis, TPR on y-axis.\n            mode='lines',                                 # Connect points with lines.\n            name=f'ROC Curve (AUC = {auc_score:.3f})',    # Label including the calculated AUC score.\n            line=dict(color='#1f77b4', width=3)           # Custom line style.\n        ))\n        \n        # Add a diagonal line representing a random classifier (AUC = 0.5).\n        fig.add_trace(go.Scatter(\n            x=[0, 1], y=[0, 1],                           # Diagonal line from (0,0) to (1,1).\n            mode='lines',\n            name='Random Classifier',                     # Label for the random classifier.\n            line=dict(color='red', dash='dash')           # Dashed red line.\n        ))\n        \n        # Update the layout of the ROC curve plot.\n        fig.update_layout(\n            title='ROC Curve',                           # Set the title of the plot.\n            xaxis_title='False Positive Rate',           # Label for the x-axis.\n            yaxis_title='True Positive Rate',            # Label for the y-axis.\n            template=\"plotly_white\"                      # Use a clean, white-themed template.\n        )\n        \n        # Save the interactive ROC curve plot as an HTML file.\n        fig.write_html(str(project_config.output_dir / \"roc_curve.html\"))\n        # fig.show() # Commented out for Notebook compatibility.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile main.py\nimport argparse # For command-line argument parsing (not directly used in Notebook but good practice for standalone scripts).\nfrom pathlib import Path # For object-oriented filesystem paths, making path manipulation robust and cross-platform.\nimport sys # For system-specific parameters and functions, used here to add current directory to Python path.\nimport json # For working with JSON data, used for saving/loading configuration and history.\nfrom loguru import logger # For professional and highly configurable logging.\nfrom rich.console import Console # From the Rich library, for beautiful and structured console output.\nfrom rich.table import Table # From Rich, for creating formatted tables in the console.\nfrom rich.panel import Panel # From Rich, for creating visually distinct panels in console output.\nfrom rich.progress import track # From Rich, for easily displaying progress bars (used implicitly by Rich when logging).\nimport tensorflow as tf # Core deep learning library by Google, fundamental for model operations.\nfrom transformers import TFBertModel # Hugging Face's TensorFlow implementation of the BERT model, crucial for loading.\n\n# Import project's custom modules, ensuring modular and organized code.\nfrom config import model_config, training_config, project_config # Project-wide configuration settings.\nfrom logger import setup_logging # Function to configure the logging system.\nfrom data_loader import DataProcessor # Class to handle data loading, cleaning, and splitting.\nfrom model import BERTModelBuilder # Class to build our BERT model architecture.\nfrom trainer import BERTTrainer # Class to encapsulate the model training process.\nfrom evaluator import ModelEvaluator # Class to handle model evaluation and visualization.\n\nconsole = Console() # Initialize a Rich Console instance for pretty console printing.\n\ndef print_project_info():\n    \"\"\"\n    Prints a formatted table summarizing the project's key components and their readiness status.\n    This provides a quick overview of the project's structure and capabilities using Rich.\n    \"\"\"\n    table = Table(title=\" Professional BERT Sentiment Analysis\")\n    table.add_column(\"Component\", style=\"cyan\", no_wrap=True) # Column for component name.\n    table.add_column(\"Status\", style=\"green\") # Column for status (e.g., \" Ready\").\n    table.add_column(\"Description\", style=\"white\") # Column for component description.\n    \n    # Add rows describing each major component of the project.\n    table.add_row(\"Data Processing\", \" Ready\", \"Advanced text cleaning and tokenization\")\n    table.add_row(\"Model Architecture\", \" Ready\", \"BERT-base with custom classification head\")\n    table.add_row(\"Training Pipeline\", \" Ready\", \"Professional training with callbacks\")\n    table.add_row(\"Evaluation Suite\", \" Ready\", \"Comprehensive metrics and visualizations\")\n    table.add_row(\"Configuration\", \" Ready\", \"Modular configuration management\")\n    table.add_row(\"Logging\", \" Ready\", \"Enhanced logging with Rich and Loguru\")\n    \n    console.print(table) # Display the formatted table to the console.\n\ndef main():\n    \"\"\"\n    The main execution pipeline for the BERT Sentiment Analysis project.\n    It orchestrates data loading, model building/loading, training, evaluation,\n    and visualization generation. This function encapsulates the entire workflow.\n    \"\"\"\n    \n    # Setup the logging system as the very first action to ensure all subsequent messages are logged.\n    setup_logging()\n    \n    # Print the project's introductory information table.\n    print_project_info()\n    \n    try:\n        logger.info(\"Starting BERT Sentiment Analysis Pipeline\")\n        \n        # Initialize the data processor, which handles IMDB dataset operations.\n        data_processor = DataProcessor()\n        \n        # Load and preprocess the IMDB dataset.\n        console.print(\"\\n[bold blue] Loading and Processing Data...[/bold blue]\")\n        # Loads 10,000 samples for efficient training/testing. This number can be adjusted in config.py.\n        texts, labels = data_processor.load_imdb_dataset(num_samples=10000) \n        \n        # Create the train, validation, and test data splits from the loaded data.\n        data_splits = data_processor.create_data_splits(texts, labels)\n\n        # Define the expected path for the saved best model.\n        model_path = project_config.models_dir / \"best_model.h5\"\n        \n        model = None # Initialize model variable.\n        history = None # Initialize history variable; will be populated if model is trained.\n\n        # Check if a trained model already exists on disk.\n        if model_path.exists():\n            console.print(f\"\\n[bold green] Found existing model at {model_path}. Loading model...[/bold green]\")\n            # Load the saved Keras model from the H5 file.\n            # `custom_objects={'TFBertModel': TFBertModel}` is crucial here. It tells Keras how to\n            # interpret and reconstruct the `TFBertModel` layer, which is a custom layer from Hugging Face\n            # and not part of standard Keras. This resolves the `ValueError: Unknown layer: 'TFBertModel'` error.\n            model = tf.keras.models.load_model(\n                str(model_path), \n                custom_objects={'TFBertModel': TFBertModel}, # Explicitly pass the custom layer.\n                compile=False # Do not compile during loading; re-compile explicitly below for evaluation.\n            )\n            # Re-compile the loaded model. This is necessary to correctly set up the optimizer and metrics\n            # for any subsequent evaluation or potential fine-tuning, even if not explicitly trained in this run.\n            model.compile(\n                optimizer=tf.keras.optimizers.Adam(learning_rate=model_config.learning_rate),\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n                metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')]\n            )\n            console.print(\"[green] Model loaded and re-compiled successfully for evaluation.[/green]\")\n        else:\n            # If no trained model is found, proceed with building and training.\n            console.print(\"\\n[bold blue] No existing model found. Building and Training BERT Model...[/bold blue]\")\n            # Build the BERT model using the predefined builder class and model configuration.\n            model = BERTModelBuilder.build_functional_model(model_config)\n            \n            console.print(f\"[green] Model built successfully![/green]\")\n            # Display the total number of trainable parameters in the model.\n            console.print(f\"Model parameters: {model.count_params():,}\") \n            \n            # Initialize the trainer with the newly built model and configurations.\n            trainer = BERTTrainer(model, model_config, training_config)\n            \n            # Start the model training process.\n            console.print(\"\\n[bold blue] Training Model...[/bold blue]\")\n            history = trainer.train(data_splits['train'], data_splits['validation'])\n            \n            # Save the training history (metrics over epochs) and model configuration.\n            trainer.save_training_artifacts()\n            console.print(f\"[green] Model trained and saved to {model_path}[/green]\")\n\n        # Ensure the `trainer` object is initialized even if the model was loaded (not trained in this run).\n        # This is important because the `evaluator` depends on `trainer.tokenizer`.\n        if model is not None and 'trainer' not in locals(): # Check if 'trainer' was not created by the 'else' block.\n            trainer = BERTTrainer(model, model_config, training_config) # Re-initialize trainer just to get the tokenizer.\n\n        # Evaluate the model on the dedicated test set.\n        console.print(\"\\n[bold blue] Evaluating Model...[/bold blue]\")\n        evaluator = ModelEvaluator(model, trainer.tokenizer) # Pass the loaded model and its tokenizer to the evaluator.\n        evaluation_results = evaluator.evaluate_model(data_splits['test'])\n        \n        console.print(Panel(f\"[green] Model evaluation complete![/green]\"))\n        console.print(\"\\n[bold yellow]Performance Metrics:[/bold yellow]\")\n        metrics_table = Table(show_header=True, header_style=\"bold magenta\")\n        metrics_table.add_column(\"Metric\", style=\"dim\")\n        metrics_table.add_column(\"Value\", style=\"bold green\")\n        for metric, value in evaluation_results['metrics'].items():\n            metrics_table.add_row(metric.replace('_', ' ').title(), f\"{value:.4f}\")\n        console.print(metrics_table)\n        \n        console.print(\"\\n[bold yellow]Classification Report:[/bold yellow]\")\n        console.print(json.dumps(evaluation_results['classification_report'], indent=2))\n        \n        # Generate and save various visualizations of the model's performance.\n        console.print(\"\\n[bold blue] Generating Visualizations...[/bold blue]\")\n        # Plot training history only if the model was actually trained in this run (i.e., `history` object exists).\n        if history: \n            evaluator.plot_training_history(history)\n            console.print(\"[green] Training history plot generated.[/green]\")\n        else:\n            console.print(\"[yellow]Skipping training history plot as model was loaded, not trained.[/yellow]\")\n\n        evaluator.plot_confusion_matrix(evaluation_results['confusion_matrix'])\n        evaluator.plot_roc_curve(evaluation_results['true_labels'], evaluation_results['predictions'])\n        console.print(\"[green] Confusion Matrix and ROC Curve plots generated.[/green]\")\n        console.print(\"[green] All visualizations saved to 'outputs' directory.[/green]\")\n        \n        console.print(Panel(\"[bold green] Project completed successfully![/bold green]\"))\n\n    except Exception as e:\n        # Catch any exceptions during the pipeline execution and log them for debugging.\n        logger.exception(f\"An error occurred during the pipeline execution: {e}\")\n        console.print(Panel(f\"[bold red] An error occurred: {e}[/bold red]\", style=\"red\"))\n\nif __name__ == \"__main__\":\n    # Ensures main() is called when the script is executed directly (not imported).\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\nimport uvicorn # ASGI server, used to run FastAPI applications.\nimport tensorflow as tf # Core deep learning library, used for loading and running the model.\nfrom fastapi import FastAPI, Request # FastAPI framework for building web APIs. `Request` is for accessing request details.\nfrom pydantic import BaseModel # Used by FastAPI for data validation and defining API request/response schemas.\nfrom typing import List, Dict, Union # For type hints, ensuring data integrity for lists, dictionaries, and flexible types.\nfrom transformers import AutoTokenizer, TFBertModel # Hugging Face components for tokenizer and BERT model in TensorFlow.\nfrom pathlib import Path # For object-oriented filesystem paths, used for model file paths.\nfrom loguru import logger # Professional logging library for structured output.\nfrom rich.console import Console # From Rich library, for visually appealing console output.\nfrom rich.panel import Panel # From Rich, for creating distinct, framed panels in console output.\nimport json # For handling JSON data, used for serialization/deserialization.\nimport re # Regular expression module for text pattern matching (used in cleaning).\nimport html # Module for decoding HTML entities in text (used in cleaning).\n\n# Import project-specific configurations and the logging setup function.\nfrom config import project_config, model_config # Access project directories and model hyperparameters.\nfrom logger import setup_logging # Function to initialize our logging system.\n\nsetup_logging() # Initialize the logging system early, ensuring all subsequent messages are captured.\nconsole = Console() # Create a Rich Console instance for custom formatted prints.\n\nclass PredictionRequest(BaseModel):\n    \"\"\"\n    Pydantic model defining the expected structure of the request body for the /predict endpoint.\n    This ensures that incoming data adheres to a specified format.\n    \"\"\"\n    texts: List[str] # A list of strings, where each string is a text for sentiment prediction.\n\nclass PredictionResponse(BaseModel):\n    \"\"\"\n    Pydantic model defining the expected structure of the response body from the /predict endpoint.\n    This provides clear documentation and validation for the API's output.\n    \"\"\"\n    predictions: List[Dict[str, Union[str, float]]] # A list of dictionaries, each containing prediction details.\n\n# Initialize the FastAPI application instance.\n# Provides metadata like title, description, and version for the API documentation (Swagger UI).\napp = FastAPI(\n    title=\"BERT Sentiment Analysis API\",\n    description=\"API for classifying movie review sentiment using a fine-tuned BERT model.\",\n    version=\"1.0.0\",\n)\n\nmodel = None # Global variable to hold the loaded TensorFlow model. Initialized to None.\ntokenizer = None # Global variable to hold the loaded BERT tokenizer. Initialized to None.\n\ndef clean_text(text: str) -> str:\n    \"\"\"\n    Cleans a single input text string by applying a series of preprocessing steps.\n    This function's implementation must be IDENTICAL to the `clean_text` function in `data_loader.py`\n    to ensure that the text processed during inference matches the format of text during training.\n    \n    Args:\n        text (str): The raw input text string to be cleaned.\n        \n    Returns:\n        str: The cleaned text string.\n    \"\"\"\n    if not isinstance(text, str): # Check if the input is actually a string.\n        return \"\" # Return an empty string if the input is not a string, to prevent errors.\n    \n    text = html.unescape(text) # Decode HTML entities (e.g., & -> &), essential for web-scraped text.\n    text = re.sub(r'<[^>]+>', '', text) # Remove any HTML tags found in the text.\n    text = re.sub(r'\\s+', ' ', text) # Normalize whitespace by replacing multiple spaces/newlines with a single space.\n    text = re.sub(r'[^\\w\\s.,!?;:-]', '', text) # Remove special characters, keeping alphanumeric, whitespace, and basic punctuation.\n    text = text.lower().strip() # Convert text to lowercase and remove leading/trailing whitespace.\n    return text\n\n@app.on_event(\"startup\")\nasync def load_model():\n    \"\"\"\n    Asynchronous function that runs ONCE when the FastAPI application starts up.\n    Its purpose is to load the pre-trained BERT model and its tokenizer into memory.\n    This prevents the model from being reloaded for every incoming prediction request,\n    which significantly improves API performance.\n    \"\"\"\n    global model, tokenizer # Declare these variables as global so they can be accessed and modified outside this function.\n    try:\n        model_path = project_config.models_dir / \"best_model.h5\" # Construct the full path to the saved model using ProjectConfig.\n        tokenizer_name = model_config.model_name # Get the pre-trained model name for tokenizer initialization from ModelConfig.\n\n        if not model_path.exists(): # Check if the model file actually exists on disk.\n            logger.error(f\"Model file not found at {model_path}. Please train the model first.\")\n            console.print(Panel(f\"[bold red] Error: Model file not found at {model_path}. Please train the model first.[/bold red]\", style=\"red\"))\n            return # Exit the function if the model file is missing, preventing further errors.\n\n        console.print(f\"[bold blue]Loading model from: {model_path}[/bold blue]\")\n        # Load the Keras model from the H5 file.\n        # `custom_objects={'TFBertModel': TFBertModel}` is CRUCIAL here. It explicitly tells Keras how to\n        # reconstruct the `TFBertModel` layer, which is a custom layer from Hugging Face Transformers.\n        # This resolves the `ValueError: Unknown layer: 'TFBertModel'` error during model loading.\n        model = tf.keras.models.load_model(\n            model_path, \n            custom_objects={'TFBertModel': TFBertModel}, \n            compile=False # Do not compile the model during loading; it will be compiled manually below.\n        ) \n        # Manually compile the loaded model. This is necessary to correctly set up the optimizer\n        # and metrics, even if we are only using the model for prediction (as model.predict() benefits).\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=model_config.learning_rate), \n                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n                      metrics=['accuracy'])\n        console.print(\"[green] Model loaded successfully![/green]\")\n\n        console.print(f\"[bold blue]Loading tokenizer: {tokenizer_name}[/bold blue]\")\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) # Load the tokenizer based on the model name.\n        console.print(\"[green] Tokenizer loaded successfully![/green]\")\n\n        logger.info(\"Model and tokenizer loaded successfully.\")\n\n    except Exception as e:\n        # Catch and log any general exceptions that occur during model or tokenizer loading.\n        logger.error(f\"Failed to load model or tokenizer: {e}\")\n        console.print(Panel(f\"[bold red] Failed to load model or tokenizer: {e}[/bold red]\", style=\"red\"))\n        model = None # Set model and tokenizer to None to indicate they are not ready.\n        tokenizer = None\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"\n    Defines a simple GET endpoint at /health.\n    This endpoint serves as a health check to verify if the API is running and if\n    the model and tokenizer have been successfully loaded into memory.\n    \n    Returns:\n        dict: A dictionary indicating the API status and model/tokenizer loading status.\n    \"\"\"\n    return {\"status\": \"ok\", \"model_loaded\": model is not None, \"tokenizer_loaded\": tokenizer is not None}\n\n@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict_sentiment(request: PredictionRequest):\n    \"\"\"\n    Defines a POST endpoint at /predict to receive text inputs and return sentiment predictions.\n    It takes a JSON request body containing a list of texts and returns a structured JSON response.\n    \n    Args:\n        request (PredictionRequest): A Pydantic model instance containing the list of texts to predict.\n        \n    Returns:\n        PredictionResponse: A Pydantic model instance containing a list of dictionaries,\n                            each with the original text, predicted sentiment label, and confidence score.\n    \"\"\"\n    # Check if the model and tokenizer are loaded. If not, return an error response.\n    if model is None or tokenizer is None:\n        logger.error(\"Model or tokenizer not loaded. Cannot process prediction.\")\n        return PredictionResponse(predictions=[{\"text\": t, \"sentiment\": \"Error: Model not loaded\"} for t in request.texts])\n\n    try:\n        console.print(f\"[bold blue]Received {len(request.texts)} texts for prediction.[/bold blue]\")\n        # Clean the input texts using the same `clean_text` function as during training.\n        cleaned_texts = [clean_text(text) for text in request.texts] \n        \n        # Tokenize the cleaned input texts.\n        # `padding='max_length'` is CRUCIAL here. It ensures that all input sequences are padded\n        # to the exact `max_length` (128) defined in the model_config, resolving the\n        # `ValueError: Input 0 ... incompatible with the layer: expected shape=(None, 128), found shape=(None, X)` error.\n        inputs = tokenizer(\n            cleaned_texts,\n            padding='max_length',  # Explicitly pads all sequences to `model_config.max_length`.\n            truncation=True,       # Truncate sequences longer than `max_length`.\n            max_length=model_config.max_length, # Uses `max_length` from config for consistency.\n            return_tensors='tf'    # Returns TensorFlow tensors for model input.\n        )\n        \n        # Make predictions using the loaded model.\n        predictions = model.predict([inputs['input_ids'], inputs['attention_mask']])\n        predicted_classes = tf.argmax(predictions, axis=1).numpy() # Get the index of the highest probability (0 or 1).\n        confidence_scores = tf.reduce_max(predictions, axis=1).numpy() # Get the probability of the predicted class.\n\n        sentiment_map = {0: \"Negative\", 1: \"Positive\"} # Map numerical predictions to human-readable labels.\n        results = []\n        # Compile results for each input text.\n        for i, text in enumerate(request.texts):\n            results.append({\n                \"text\": text,\n                \"sentiment\": sentiment_map[predicted_classes[i]],\n                \"confidence\": float(confidence_scores[i]) # Convert NumPy float to standard Python float for JSON serialization.\n            })\n        logger.info(f\"Successfully predicted sentiment for {len(request.texts)} texts.\")\n        return PredictionResponse(predictions=results) # Return the structured prediction response.\n\n    except Exception as e:\n        # Catch and log any errors that occur during the prediction process.\n        logger.error(f\"Error during prediction: {e}\", exc_info=True) # `exc_info=True` logs the full traceback.\n        # Return an error message in the response to the client.\n        return PredictionResponse(predictions=[{\"text\": t, \"sentiment\": f\"Error: {e}\"} for t in request.texts])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Installing dependencies from requirements.txt...\")\n!pip install -r requirements.txt\nprint(\"Dependencies installation complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Setting up ngrok...\") # Informative print statement indicating the start of ngrok setup.\nimport os # Module for interacting with the operating system (used for file path checks and environment variables).\nimport time # For time-related functions (though not directly used for delays in this specific cell).\n\n# ====== IMPORTANT: INSTALL pyngrok AND DOWNLOAD/CONFIGURE ngrok EXECUTABLE HERE ======\n# This section ensures that pyngrok (the Python wrapper) is installed and\n# the ngrok executable binary is downloaded and configured in the Kaggle environment.\n# It's placed at the top of this cell so `pyngrok` is available before any import statement\n# that uses it within this same cell, preventing ModuleNotFoundError.\n\nprint(\"Installing pyngrok...\") # Inform the user that pyngrok is being installed.\n!pip install pyngrok -q # Install the pyngrok Python library. `-q` means quiet output.\nprint(\"pyngrok installed.\") # Confirmation message.\n\n# Define paths for the ngrok zip file and the executable itself.\nngrok_zip_path = \"ngrok-stable-linux-amd64.zip\"\nngrok_executable_path = \"ngrok\"\n\n# Check if the ngrok executable already exists to avoid re-downloading unnecessarily.\nif not os.path.exists(ngrok_executable_path):\n    print(\"Downloading ngrok executable...\") # Inform the user about the download.\n    # Download the ngrok executable zip file for Linux (Kaggle's environment).\n    # `-O` specifies the output filename. `-q` is for quiet download progress.\n    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O {ngrok_zip_path} -q \n    !unzip -o {ngrok_zip_path} # Unzip the downloaded file. `-o` means overwrite if already exists.\n    !chmod +x {ngrok_executable_path} # Grant execute permissions to the unzipped ngrok binary.\n    print(\"Ngrok executable downloaded and configured.\") # Confirmation message.\nelse:\n    print(\"Ngrok executable already present.\") # Inform if ngrok is already there.\n\n# ====================================================================================\n\n# Now, import ngrok AFTER it's confirmed installed/present in the environment.\nfrom pyngrok import ngrok # Import the pyngrok library, which provides Python bindings for ngrok.\n\n# Kill any residual ngrok processes or tunnels from previous sessions.\n# This is crucial for free ngrok accounts as they are limited to 1 simultaneous ngrok agent session (ERR_NGROK_108).\n# It ensures a clean slate before attempting to open a new tunnel.\nngrok.kill() \nprint(\"Killed any residual ngrok processes.\") # Confirmation message.\n\n# Authenticate ngrok with your personal authentication token.\n# This token is required for using ngrok's service and creating public tunnels under your account.\nngrok_auth_token = \"2xgYVbiMX6umq43cgdupTkDYOoA_6sEh5wdWfe7YBsTq7HhGf\" # REPLACE this with your actual ngrok auth token from ngrok.com dashboard.\n\n# Validate if the authentication token has been set correctly.\nif ngrok_auth_token == \"YOUR_NGROK_AUTH_TOKEN\" or not ngrok_auth_token:\n    print(\" Warning: Please replace 'YOUR_NGROK_AUTH_TOKEN' with your actual ngrok auth token from ngrok.com\")\n    # Raise a ValueError if the token is not set, as ngrok will fail without it.\n    raise ValueError(\"Ngrok auth token is not set. Cannot start tunnel.\") \nelse:\n    # Set the ngrok authentication token as an environment variable.\n    os.environ[\"NGROK_AUTH_TOKEN\"] = ngrok_auth_token \n    # Authenticate pyngrok with the provided auth token. This connects your pyngrok client to your ngrok account.\n    ngrok.set_auth_token(ngrok_auth_token) \n    print(\"Ngrok auth token set and authenticated.\") # Confirmation message.\n\nprint(\"Ngrok setup complete. Ready to start tunnel.\") # Final confirmation for the cell.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys # Import the 'sys' module to interact with the Python runtime environment.\n\n# Add the current working directory ('/kaggle/working/') to Python's system path.\n# This is crucial for Python to be able to find and import our custom modules\n# (like 'config', 'logger', 'data_loader', 'model', 'trainer', 'evaluator', 'main')\n# that are written to this directory using `%%writefile`.\nif '/kaggle/working/' not in sys.path: # Check if the path is not already added to avoid duplicates.\n    sys.path.append('/kaggle/working/') \n\n# Import the 'main' function from our 'main.py' script.\n# This function encapsulates the entire BERT sentiment analysis pipeline.\nfrom main import main\n\nprint(\"Running main.py to ensure model is saved (will train if not found)...\") # Informative message for the user.\n\n# Execute the 'main' function.\n# This call orchestrates the core logic:\n# - It first checks if a trained model ('best_model.h5') exists.\n# - If found, it loads the model. This step is optimized to avoid redundant training.\n# - If not found, it triggers the full training process and saves the model.\n# - Afterwards, it proceeds with model evaluation and visualization generation.\nmain()\n\nprint(\"main.py finished. Model should now be saved.\") # Confirmation message after the pipeline completes.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This cell is designed to start the FastAPI application and expose it to the internet\n# using an ngrok public tunnel. It integrates Uvicorn (the ASGI server) with ngrok.\n\nprint(\"Starting FastAPI application...\") # Informative message for the user.\n\nimport os # Module for interacting with the operating system (e.g., managing environment variables).\nimport threading # Module for running tasks in separate threads, allowing the notebook cell to remain active.\nfrom IPython.display import display, HTML # For displaying HTML content (like clickable links) in the notebook output.\nimport time # For pausing execution (e.g., to allow services to start).\nimport subprocess  # For running external commands (like Uvicorn) as subprocesses.\n\n# ====== IMPORTANT: This section ensures pyngrok is installed and ngrok executable is ready. ======\n# It is placed here for robustness, although usually handled in a dedicated setup cell (Cell 12).\n# If Cell 12 consistently runs first, these lines might be redundant but harmless.\n\nprint(\"Installing pyngrok...\") # Inform the user that pyngrok is being installed.\n!pip install pyngrok -q # Install the pyngrok Python library. `-q` ensures quiet output.\n\n# Define paths for the ngrok zip file and the executable.\nngrok_zip_path = \"ngrok-stable-linux-amd64.zip\"\nngrok_executable_path = \"ngrok\"\n\n# Check if the ngrok executable already exists to avoid re-downloading.\nif not os.path.exists(ngrok_executable_path):\n    print(\"Downloading ngrok executable...\") # Inform the user about the download.\n    # Download the ngrok executable zip file for Linux (Kaggle's environment).\n    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O {ngrok_zip_path} -q \n    !unzip -o {ngrok_zip_path} # Unzip the downloaded file. `-o` overwrites if exists.\n    !chmod +x {ngrok_executable_path} # Grant execute permissions to the ngrok binary.\n    print(\"Ngrok executable downloaded and configured.\") # Confirmation message.\nelse:\n    print(\"Ngrok executable already present.\") # Inform if ngrok is already there.\n# =================================================================================================\n\n# Import ngrok only after ensuring its installation and executable are ready.\nfrom pyngrok import ngrok # pyngrok is the Python wrapper for the ngrok service.\n\n# Retrieve your ngrok authentication token. This token is essential for ngrok to operate.\n# It should have been set as an environment variable in a previous setup cell (Cell 12).\nngrok_auth_token = \"2xgYVbiMX6umq43cgdupTkDYOoA_6sEh5wdWfe7YBsTq7HhGf\" # REPLACE this with your actual ngrok auth token.\n\n# Validate the ngrok auth token. If it's not set, raise an error as ngrok won't function.\nif ngrok_auth_token == \"YOUR_NGROK_AUTH_TOKEN\" or not ngrok_auth_token:\n    print(\" Warning: Please replace 'YOUR_NGROK_AUTH_TOKEN' with your actual ngrok auth token from ngrok.com\")\n    print(\"Register for free at https://ngrok.com/signup and get your token from https://dashboard.ngrok.com/get-started/your-authtoken\")\n    raise ValueError(\"Ngrok auth token is not set. Cannot start tunnel.\") \nelse:\n    os.environ[\"NGROK_AUTH_TOKEN\"] = ngrok_auth_token # Set the token as an environment variable.\n    ngrok.set_auth_token(ngrok_auth_token) # Authenticate pyngrok with the provided auth token.\n\n# Define a function to run the Uvicorn server. This function will be executed in a separate thread.\ndef run_uvicorn():\n    \"\"\"\n    Runs the Uvicorn server in a subprocess to host the FastAPI application.\n    Captures Uvicorn's output for monitoring.\n    \"\"\"\n    try:\n        # Command to start Uvicorn, serving the 'app' FastAPI instance from 'app.py' module\n        # on all network interfaces (0.0.0.0) and port 8000.\n        command = [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n        \n        # Use subprocess.Popen to run Uvicorn. `Popen` is non-blocking, allowing the main thread to continue.\n        # `stdout=subprocess.PIPE` and `stderr=subprocess.PIPE` capture Uvicorn's console output.\n        # `text=True` decodes output as text.\n        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        \n        # Read Uvicorn's output line by line to detect when the server has fully started.\n        for line in process.stdout:\n            print(f\"Uvicorn: {line.strip()}\") # Print Uvicorn's output to the notebook.\n            if \"Uvicorn running on\" in line: # Look for a specific pattern indicating the server is active.\n                print(\"Uvicorn server confirmed to be starting...\") # Confirm Uvicorn startup.\n                break # Exit the loop once the server is confirmed to be starting, to proceed with ngrok.\n        \n        # Wait for the Uvicorn process to complete. For a server, this means it will run indefinitely\n        # unless it crashes or is explicitly stopped. This makes the thread (and thus the cell) remain active.\n        process.wait()\n\n    except FileNotFoundError:\n        print(\"Error: 'uvicorn' command not found. Ensure uvicorn is installed (from requirements.txt).\")\n    except Exception as e:\n        print(f\"Error in Uvicorn thread: {e}\")\n\ntry:\n    # Create and start a new thread to run the `run_uvicorn` function.\n    # This allows the FastAPI server to run in the background without blocking the notebook cell.\n    uvicorn_thread = threading.Thread(target=run_uvicorn)\n    uvicorn_thread.daemon = True # Set the thread as a daemon; it will terminate when the main program exits.\n    uvicorn_thread.start() # Start the Uvicorn thread.\n\n    time.sleep(15) # Pause execution to allow time for Uvicorn to start and the FastAPI app (including model loading) to initialize.\n    \n    # Establish the ngrok tunnel. This exposes the locally running Uvicorn server (on port 8000)\n    # to the internet, providing a public, accessible URL.\n    public_url = ngrok.connect(8000).public_url\n    print(f\"Ngrok Tunnel URL: {public_url}\") # Print the generated public URL.\n    # Display a clickable HTML link to the FastAPI documentation (Swagger UI).\n    display(HTML(f'<h2>Your FastAPI app is running at: <a href=\"{public_url}/docs\" target=\"_blank\">{public_url}/docs</a></h2>'))\n    print(\"Uvicorn server started in a background thread.\") # Confirmation message.\n\n    uvicorn_thread.join() # Keep the notebook cell alive indefinitely while the Uvicorn server thread is active.\n\nexcept Exception as e:\n    # Catch any exceptions during ngrok or Uvicorn startup and log them.\n    print(f\" Error starting ngrok or Uvicorn: {e}\")\n    print(\"Please check your ngrok auth token and ensure no other process is using port 8000.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T17:26:57.566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests # Library for making HTTP requests (e.g., GET, POST) to web services/APIs.\nimport json # For working with JSON data (serializing Python objects to JSON, parsing JSON responses).\nimport time # For pausing execution (e.g., to wait for the API to start up).\nfrom rich.console import Console # From Rich library, for beautiful and structured console output.\nfrom rich.panel import Panel # From Rich, for displaying visually distinct panels in console output.\n\nconsole = Console() # Initialize a Rich Console instance for formatted printing to the notebook output.\n\n# ====== IMPORTANT: Update this URL with the actual Ngrok Tunnel URL displayed by the previous cell. ======\n# The ngrok URL is temporary and changes each time the tunnel is established (i.e., every time Cell 14 is run).\n# You MUST copy the NEW URL from the output of Cell 14 and paste it here before executing this cell.\nngrok_url = \"https://f085-146-148-39-26.ngrok-free.app\" # Placeholder: REPLACE THIS LINE with the NEW Ngrok URL!\n# Example of finding the URL: Look for \"Ngrok Tunnel URL:\" in the output of the previous cell (Cell 14).\n# =========================================================================================================\n\nconsole.print(f\"[bold blue]Testing API at: {ngrok_url}[/bold blue]\") # Inform the user about the API URL being tested.\n\n# Pause execution to give the FastAPI application (running via Uvicorn and Ngrok)\n# sufficient time to fully start up and load the BERT model.\n# This duration might need adjustment based on model size, internet speed, and Kaggle's GPU/CPU speed.\ntime.sleep(20) \n\ntry:\n    # Send a GET request to the /health endpoint of the API.\n    # This checks if the API is running and if the model/tokenizer are loaded.\n    health_response = requests.get(f\"{ngrok_url}/health\")\n    console.print(f\"Health Check Status: {health_response.status_code}\") # Print the HTTP status code (e.g., 200 OK).\n    console.print(f\"Health Check Response: {health_response.json()}\") # Print the JSON response from the health check.\n    \n    # Extract the 'model_loaded' status from the health check response.\n    model_loaded = health_response.json().get(\"model_loaded\", False)\n    if not model_loaded:\n        # If the model is not reported as loaded by the health check, display a warning.\n        console.print(Panel(\"[bold yellow]Warning: Model not yet loaded according to health check. Prediction might fail.[/bold yellow]\", style=\"yellow\"))\nexcept requests.exceptions.ConnectionError as e:\n    # Catch a ConnectionError, which means the API could not be reached.\n    # This often indicates that ngrok or Uvicorn is not running, or the URL is incorrect.\n    console.print(Panel(f\"[bold red]Error: Could not connect to the API. Make sure ngrok and uvicorn are running. Error: {e}[/bold red]\", style=\"red\"))\n    health_response = None # Set response to None to prevent further processing.\n    model_loaded = False # Indicate that the model is not loaded due to connection failure.\nexcept json.JSONDecodeError as e: \n    # Catch a JSONDecodeError. This means the API responded, but its content was not valid JSON.\n    # This can happen if the API returns an HTML error page (e.g., 404 Not Found) instead of JSON.\n    console.print(Panel(f\"[bold red]Error: Could not decode JSON response from health check. Error: {e}[/bold red]\", style=\"red\"))\n    health_response = None\n    model_loaded = False\n\n\n# Proceed to send prediction requests only if the health check passed (status 200)\n# and the model was confirmed as loaded.\nif health_response and health_response.status_code == 200 and model_loaded:\n    # Define a list of sample texts to send to the API for sentiment prediction.\n    test_texts = [\n        \"This movie was absolutely fantastic! I loved every moment of it.\",\n        \"The plot was confusing and the acting was terrible. A complete waste of time.\",\n        \"It was an okay movie, nothing special, but not bad either.\",\n        \"What a masterpiece! Highly recommend.\"\n    ]\n\n    headers = {\"Content-Type\": \"application/json\"} # Set HTTP header to indicate the request body is JSON.\n    data = {\"texts\": test_texts} # Prepare the request body as a Python dictionary.\n\n    console.print(\"\\n[bold blue]Sending prediction request...[/bold blue]\") # Inform the user about sending the prediction request.\n    try:\n        # Send a POST request to the /predict endpoint.\n        # `data=json.dumps(data)` converts the Python dictionary to a JSON string for the request body.\n        predict_response = requests.post(f\"{ngrok_url}/predict\", headers=headers, data=json.dumps(data))\n        console.print(f\"Prediction Status: {predict_response.status_code}\") # Print the HTTP status code for prediction.\n        console.print(\"Prediction Response:\") # Label for the prediction response.\n        console.print(json.dumps(predict_response.json(), indent=2)) # Parse and print the JSON prediction response, pretty-printed.\n        console.print(Panel(\"[bold green] Prediction successful![/bold green]\")) # Confirm successful prediction.\n    except requests.exceptions.ConnectionError as e:\n        # Handle connection errors during the prediction request.\n        console.print(Panel(f\"[bold red]Error: Could not connect to the prediction endpoint. Error: {e}[/bold red]\", style=\"red\"))\n    except Exception as e:\n        # Catch any other general errors during the prediction request.\n        console.print(Panel(f\"[bold red]Error during prediction request: {e}[/bold red]\", style=\"red\"))\nelse:\n    # If the API is not ready (e.g., model not loaded, health check failed), inform the user.\n    console.print(Panel(\"[bold red]API is not fully ready for predictions. Check previous output for errors.[/bold red]\", style=\"red\"))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T17:26:57.566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}